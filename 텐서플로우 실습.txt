설치 참고 https://meisteruser.net/devflow/1863
삭제 rm -r ~/tensorflow

텐서플로우 실행하기
source ~/tensorflow/bin/activate

비활성화 : deactivate

Python 에서 import 하는 법 
Import tensor flow as tf


Data flow graph => 노드들이 연산(operations)이고 엣지는 데이터(tensors)
어떤 연산이 일어나서 내가 원하는 결과물을 얻거나 작업을 할 수 있는 것


https://github.com/hunkim/DeepLearningZeroToAll
1. 머신러닝의 개념과 용어


노드 만들어서 실행하는 방법
노드이름 = tf.constant(노드값) 노드 만드는 것
세션이름 = tf.Session() 세션 만들고
print(세션이름.run(실행시키고자 하는 노드)) 실행


그래프 만들어 놓고 실행시키는 단계에서 값을 던져주고 싶다
 => 노드를 placeholer로 만들어 주고 feed_dict 로 값을 넘겨준다

a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.타입)
adder_node = a+b
print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))
print(sess.run(adder_node, feed_dict={a: [3,4], b: [4,5]})) => 값은 array도 가능
이때 값 지정

Tensor 는 rank, shapes, types
1) rank
Rank 
0			a=123
1		a=[1,2,3]
2			a = [[1,2],[3,4]]
n

2) shapes : 각각 element에 몇개 들어있는지
T = [3,4] => [D0,D1] 또는 (0 1)
T = [[1,2,3],[4,5,6]] => (2 3) 또는 [2,3]

3) types
tf.float32 젤 많이 사용
tf.int32 그 다음 많이 사용



2. Linear Regression 의 개념

x_train = [1,2,3]
y_train = [1,2,3]

H(x) = Wx + b
W = tf.Variable(tf.random_normal([1]), name = ‘weight’
b = tf.Variable(tf.random_normal(shape을 주면 됨), name=‘bias’
hypothesis = x_train * W + b

Cost = tf.reduce_mean(tf.square(hypothesis - y_train))
*reduce_mean => 평균 내주는 것
*square -> 제곱

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)
자동으로 미분하고 최소값
미분하고 얼만큼 움직일지 곱한다음에 원래 기울기(W)에서 그 만큼 빼주는 라이브러리
learnig_rate => 얼만큼 움직일지

sess = tf.Session()
varaiable 사용 전에는 sess.run(tf.global_variables_initalizer())

for setp in range(2001):
	sess.run(train)
	if setp % 20 == 0:
		print(step, sess.run(cost), sess.run(W), sess.run(b))

2) placeholders
shape = [None] 아무 값이나 원하는 개수만큼




3. Linear Regression cost 함수 최소화

w가 함수에서 x값 이라고 해야하나
cost함수의 최소값을 갖는 w 값 찾기

a.assign(b) => a 를 b로 업데이트
cost = tf.reduce_mean()
optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01) => 자동으로 미분해줌
train = optimizer.minimize(cost) 
미분하고 얼만큼 움직일지 곱한다음에 원래 기울기에서 그 만큼 빼주는 라이브러리

+)
gvs = optimizer.compute_gradients(cost) => cost의 gradients(기울기) 값을 계산 수정가능
*compute_gradients(cost) => cost의 기울기 자동으로 계산
apply_gradients = optimizer.apply_gradients(gvs)



4. 여러개의 입력(feature)의 Linear Regression

데이터를 파일에서 읽어오기 => import numpy as np
csv 를 주로 사용

데이터가 모두 같은 타입 이어야 사용 가능
xy = np.loadtxt(‘파일이름.csv', delimiter=‘,’, dtype=np.float32)

파이썬 배열 slicing 많이 쓰임

파일이 더 크면 Queue Runners 
여러 개의 파일을 읽으면 큐에 저장하고 리더를 사용해서 큐에 나눠서 저장
filename_queue = tf.train.string_input_producer(['data-01-test-score.csv’, ‘hi.csv’], shuffle=False, name='filename_queue')

reader = tf.TextLineReader() => 리더 정의 
key, value = reader.read(filename_queue)

record_defaults = [[0.], [0.], [0.], [0.]] => 데이터 타입이 어떻다는 것을 넘겨줌
xy = tf.decode_csv(value, record_defaults=record_defaults) => value를 어떻게 parsing 할 것 인지 

train_x_batch, train_y_batch = \
    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10) => 데이터를 읽어올 수 있게 해주는 것, 몇개 씩 가져오는지가 batch_size




5. Logistic (Regression) Classification - 정확도 높은 알고리즘 소개

tf.sigmoid
tf.cast => true = 1, false = 0

predicted => 예측 값이 0.5 보다 크면 1 작으면 0
accuracy => 정확도 평균



6. Softmax Regression (Multinomial Logistic Regression)

hypothesis 예측값 = tf.nn.softmax(tf.matmul(X,W)+b => score값)

softmax 함수는 나오는 결과 값들을 확률로 바꿔줌. 그래서 나온 값들의 총 합은 1이 됨

cost(loss) function = 	tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1)) 	합을 하고 평균을 낸다
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

one-hot-encoding : 제일 큰 값만 1로 하고 나머지는 0으로 나타내는 것
nb_classes => 클래스가 몇개인가

sess.run(tf.global_variables_initializer()) 초기화하고 실행

tf.arg_max(a, 1) array 주면 첫번째로 큰 값을 돌려줌


softmax_cross_entropy_with_logits
logits = tf.matmul(X,W) + b
hypothesis = tf.nn.softmax(logits)

cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y_one_hot)
항상 logits을 넘긴다는 것을 기억!
이것을 사용하면 전에 썼던 -어쩌구 로그 어쩌구 계산을 쉽게 할 수 있음
cost = tf.reduce_mean(cost_i)

y_one_hot = tf.one_hot(y, nb_classes) => 	데이터 y가 몇개의 클래스 가지고 있는지 알려줘야 one_hot으로 만들 수 있음, 차원이 한개 늘어남 그래서
y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes]) => -1은 모든것을 나타냄, 클래스 개수
reshape으로 맞춰줘야 함

w 는 입력, 출력 개수
b 는 출력개수
tf.random_normal?

prediction = tf.argmax(hypotheis, 1) => hypothesis를 레벨로 바꿈. 보기 중 1개 값으로
correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))  => 두가지가 맞는지 비교
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) => 예측 값을 모아서 평균을 냄. 정확도

Cast 함수는 뒤에 주는 형태로 변환해줌 여기서는 true = 1, false = 0 이렇게

y data.flatten => [[1], [0]] 을 하나로. [1,0] 이렇게 해준다
for p, y in zip (pred, y data.flatten()): => 각 리스트를 묶어서 p, y로 보낸다




7. ML의 실용과 몇가지 팁


우리가 가지고 있는 데이터를 training 과 test 로 나눈다
prediction, accuracy를 나타낼 때는 test data로

normalized inputs => MinMaxScaler(데이터) 
def MinMaxScaler(data):

    numerator = data - np.min(data, 0)

    denominator = np.max(data, 0) - np.min(data, 0)

    # noise term prevents the zero division

    return numerator / (denominator + 1e-7)


batch_xs, batch_ys = mnist.train.next_batch(batch_size)  전체 데이터 set을 batch size 만큼 올려서 학습
얼만큼 돌지는 total_batch = int(minst.train.num_examples / batch_size)
total_batch를 다 돌면 epoch한번이 끝난 것

epoch => 전체 데이터set을 한번 돌리는 것
정확도 이렇게 할 수도 있음 sess.run 안하고 하는 방법
print("Accuracy: ", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))



8. 딥러닝의 기본 개념과 문제, 그리고 해결

rank => []개수
shape => rank개수만큼 나옴 가장 안쪽 괄호로 들어가서 몇개인지 세고 나오면서 그 묶인게 몇개인지 세고 그렇게 개수를 적을 떄는 끝에서부터 적어야 함 [[1],[2]] => [2,1]
axis => 축. shape의 개수만큼 0부터 시작 젤 바깥이 0 안쪽으로 들어감

행렬곱셈은

1)broadcasting => shape이 달라도 연산이 가능하게 해줌 shape을 맞춰주는,,,

2)tf.argmax 는 가장 큰 값의 위치

3)tf.reshape  할때 원래 shpae의 가장 끝 값은 건들이지 않는다
squeeze => 펴준다	expand는 쪼개준다

4) tf.one_hot
가장 큰 값만 1로 나머지 0으로 

5) tf.cast  타입을 하나로 통일

6) tf.stack  배열을 어떻게 쌓을지

7) tf.ones_like(배열이름).eval() 배열이름과 같은 모양의 값이 모두 1인 걸 만들어줌

8) tf.zeros_like(x).eval() 이거는 0으로 만들어줌

9) zip 배열을 한번에 받아서 처리
for x, y in zip([1,2,3], [4,5,6]):

Epoch => 전체 다 돌리면 1 epoch 전체 데이터를 몇번 돌리냐
Batch => 한번에 돌릴 데이터 크기? 사이즈?



9. Neural Network 1 : XOR 문제와 학습방법


Logistic regression 사용  결과는 1, 0 이니까

Deep 하게 쌓는 것은 결과 값을 받아와서 여러번 계산한다

중간단계 출력 개수는 우리가 정하면 됨

Wide NN 은 중간 출력 개수를 크게 지정해주는 건데 그러면 더 정확해짐
Deep NN 은 깊게 계산을 여러번 쌓는 것임. 역시 더 정확해짐


Tensor board 
K
어떤 것 기록할지
tf.summary.histogram
tf.summary.scaler

tf.summary.merge_all()

Writer = tf.summary.FileWriter(‘파일이름’)
writer.add_graph(sess.graph) 그래프 추가


run할 때
summary 를 같이 실행 s 로 받음
writer.add_summary(s, global_step = global_step)
global_steop += 1

Graph로 보고싶다 할 때는 tf.name_scope(“ “)as scope: 
이렇게하면 각 레이어 별로 정ㅇ리가 됨

그리고  learning_rate 에 변화를 주고 한번에 보고싶으면 /board/1, /board/2  이런식으로 하고
/board로 탠서보드를 실행시키면 한눈에 비교 가능
















11. CNN

입력된 이미지를 convolution으로 값을 뽑아내고 subsampling 데이터를 줄여주고
이것을 여러번 반복하고 마지막 값을 classification 등으로 결과를 냄

두가지로 나눌 수 있음 convolution layer and max pooling

Plit.imageshow 는 그냥 이미지 시각화

Image: 1,3,3,1  처음은 몇개의 이미지 그다음은 이미지 크기 그다음은 칼라 만약에 흑백이 아니면 3이 됨
Filter : 2,2,1,1 이런식인데 앞에 두자리는 크기, 그 다음은 칼라, 그 다음은 필터개수 = 몇번 하겠따 weight 값에 쓸떄 사용
Strides = [1,2,2,1] 가운데 사이즈로 옮긴다..? 위 아래 2,2 만 보면 됨

tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding=‘VALID’) 하면 자동으로 해줌
실행은 conv2d_img = conv2d.eval()

Padding 옵션 same => 필터에 적용한 결과 사이즈가 뭐든지 원래 이미지 사이즈와 같게 만들어 주겠다. 입력사이즈 = 출력사이즈 가 됨

Max pooling
주어진 이미지에 대해서 알아서 뽑아줌 ksize, strides, padding 만 주면 됨
ksize = [1, 2, 2, 1] 여기서 실제 크기는 2*2
strides = [1,2,2,1]


마지막에는 쭉 펼쳐줌 그다음 fully connected (fc) 에 넣어줌
원래 하던 softmax 이런거

tf.layers.conv2d 하면 좀 더 쉽게 사용 가능
tf.layers.dense 는 weight 갯수 계산 안해도 됨

Ensemble 은 여러개 모델을 예측시켜보고 조합시킨다